{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "016D036D46DD4C408D48EFA828B4E77C"
   },
   "source": [
    "# Batch Sinkhorn Iteration Wasserstein Distance\n",
    "\n",
    "Thomas Viehmann\n",
    "\n",
    "This notebook implements sinkhorn iteration wasserstein distance layers.\n",
    "\n",
    "## Important note: This is under construction and does not yet work as well as it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "4FA01C447B594B37B4828D7A42DE20EB",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "604CDE3AD9E94ECD8EAF9F423C06744D"
   },
   "source": [
    "The following is a \"plain sinkhorn\" implementation that could be used in\n",
    "[C. Frogner et. al.: Learning with a Wasserstein Loss](https://arxiv.org/abs/1506.05439)\n",
    "\n",
    "Note that we use a different convention for $\\lambda$ (i.e. we use $\\lambda$ as the weight for the regularisation, later versions of the above use $\\lambda^-1$ as the weight).\n",
    "\n",
    "The implementation has benefitted from\n",
    "\n",
    "- Chiyuan Zhang's implementation in [Mocha](https://github.com/pluskid/Mocha.jl),\n",
    "- Rémi Flamary's implementation of various sinkhorn algorithms in [Python Optimal Transport](https://github.com/rflamary/POT)\n",
    "\n",
    "Thank you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "E3421C815A6B4EB1B47A95F053B78BFF",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WassersteinLossVanilla(torch.autograd.Function):\n",
    "    def __init__(self,cost, lam = 1e-3, sinkhorn_iter = 50):\n",
    "        super(WassersteinLossVanilla,self).__init__()\n",
    "        \n",
    "        # cost = matrix M = distance matrix\n",
    "        # lam = lambda of type float > 0\n",
    "        # sinkhorn_iter > 0\n",
    "        # diagonal cost should be 0\n",
    "        self.cost = cost\n",
    "        self.lam = lam\n",
    "        self.sinkhorn_iter = sinkhorn_iter\n",
    "        self.na = cost.size(0)\n",
    "        self.nb = cost.size(1)\n",
    "        self.K = torch.exp(-self.cost/self.lam)\n",
    "        self.KM = self.cost*self.K\n",
    "        self.stored_grad = None\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"pred: Batch * K: K = # mass points\n",
    "           target: Batch * L: L = # mass points\"\"\"\n",
    "        assert pred.size(1)==self.na\n",
    "        assert target.size(1)==self.nb\n",
    "\n",
    "        nbatch = pred.size(0)\n",
    "        \n",
    "        u = self.cost.new(nbatch, self.na).fill_(1.0/self.na)\n",
    "        \n",
    "        for i in range(self.sinkhorn_iter):\n",
    "            v = target/(torch.mm(u,self.K.t())) # double check K vs. K.t() here and next line\n",
    "            u = pred/(torch.mm(v,self.K))\n",
    "            #print (\"stability at it\",i, \"u\",(u!=u).sum(),u.max(),\"v\", (v!=v).sum(), v.max())\n",
    "            if (u!=u).sum()>0 or (v!=v).sum()>0 or u.max()>1e9 or v.max()>1e9: # u!=u is a test for NaN...\n",
    "                # we have reached the machine precision\n",
    "                # come back to previous solution and quit loop\n",
    "                raise Exception(str(('Warning: numerical errrors',i+1,\"u\",(u!=u).sum(),u.max(),\"v\",(v!=v).sum(),v.max())))\n",
    "\n",
    "        loss = (u*torch.mm(v,self.KM.t())).mean(0).sum() # double check KM vs KM.t()...\n",
    "        grad = self.lam*u.log()/nbatch # check whether u needs to be transformed        \n",
    "        grad = grad-torch.mean(grad,dim=1, keepdim=True)\n",
    "        grad = grad-torch.mean(grad,dim=1, keepdim=True) # does this help over only once?\n",
    "        self.stored_grad = grad\n",
    "\n",
    "        dist = self.cost.new((loss,))\n",
    "        return dist\n",
    "    def backward(self, grad_output):\n",
    "        #print (grad_output.size(), self.stored_grad.size())\n",
    "        return self.stored_grad*grad_output[0],None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "33E33D188E5D4F5A8E61D23599F01A6A"
   },
   "source": [
    "The following is a variant of the \"log-stabilized sinkhorn\" algorithm as described by [B. Schmitzer: Stabilized Sparse Scaling Algorithms for Entropy Regularized Transport Problems](https://arxiv.org/abs/1610.06519).\n",
    "However, the author (for his application of computing the transport map for a single pair of measures) uses a form that modifies the $K$ matrix. This makes is less suitable for processing (mini-) batches, where we want to avoid the additional dimension.\n",
    "\n",
    "To the best of my knowledge, this is the first implementation of a batch stabilized sinkhorn algorithm and I would appreciate if you find it useful, you could credit\n",
    "*Thomas Viehmann: Batch Sinkhorn Iteration Wasserstein Distance*, [https://github.com/t-vi/pytorch-tvmisc/wasserstein-distance/Pytorch_Wasserstein.ipynb](https://github.com/t-vi/pytorch-tvmisc/wasserstein-distance/Pytorch_Wasserstein.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "C12F43EA5C8F4519841D74209DDF5F0B",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WassersteinLossStab(torch.autograd.Function):\n",
    "    def __init__(self,cost, lam = 1e-3, sinkhorn_iter = 50):\n",
    "        super(WassersteinLossStab,self).__init__()\n",
    "        \n",
    "        # cost = matrix M = distance matrix\n",
    "        # lam = lambda of type float > 0\n",
    "        # sinkhorn_iter > 0\n",
    "        # diagonal cost should be 0\n",
    "        self.cost = cost\n",
    "        self.lam = lam\n",
    "        self.sinkhorn_iter = sinkhorn_iter\n",
    "        self.na = cost.size(0)\n",
    "        self.nb = cost.size(1)\n",
    "        self.K = torch.exp(-self.cost/self.lam)\n",
    "        self.KM = self.cost*self.K\n",
    "        self.stored_grad = None\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"pred: Batch * K: K = # mass points\n",
    "           target: Batch * L: L = # mass points\"\"\"\n",
    "        assert pred.size(1)==self.na\n",
    "        assert target.size(1)==self.nb\n",
    "\n",
    "        batch_size = pred.size(0)\n",
    "        \n",
    "        log_a, log_b = torch.log(pred), torch.log(target)\n",
    "        log_u = self.cost.new(batch_size, self.na).fill_(-numpy.log(self.na))\n",
    "        log_v = self.cost.new(batch_size, self.nb).fill_(-numpy.log(self.nb))\n",
    "        \n",
    "        for i in range(self.sinkhorn_iter):\n",
    "            log_u_max = torch.max(log_u, dim=1, keepdim=True).values\n",
    "            u_stab = torch.exp(log_u - log_u_max)\n",
    "            log_v = log_b - torch.log(torch.mm(self.K.t(),u_stab.t()).t()) - log_u_max\n",
    "            log_v_max = torch.max(log_v, dim=1, keepdim=True).values\n",
    "            v_stab = torch.exp(log_v - log_v_max)\n",
    "            log_u = log_a - torch.log(torch.mm(self.K, v_stab.t()).t()) - log_v_max\n",
    "\n",
    "        log_v_max = torch.max(log_v, dim=1, keepdim=True).values\n",
    "        v_stab = torch.exp(log_v - log_v_max)\n",
    "        logcostpart1 = torch.log(torch.mm(self.KM, v_stab.t()).t())+log_v_max\n",
    "        wnorm = torch.exp(log_u+logcostpart1).mean(0).sum() # sum(1) for per item pair loss...\n",
    "        grad = log_u*self.lam\n",
    "        grad = grad - grad.mean(dim=1, keepdim=True)\n",
    "        grad = grad - grad.mean(dim=1, keepdim=True) # does this help over only once?\n",
    "        grad = grad/batch_size\n",
    "        \n",
    "        self.stored_grad = grad\n",
    "\n",
    "        return self.cost.new((wnorm,))\n",
    "    def backward(self, grad_output):\n",
    "        #print (grad_output.size(), self.stored_grad.size())\n",
    "        #print (self.stored_grad, grad_output)\n",
    "        res = grad_output.new()\n",
    "        res.resize_as_(self.stored_grad).copy_(self.stored_grad)\n",
    "        if grad_output[0] != 1:\n",
    "            res.mul_(grad_output[0])\n",
    "        return res,None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "076665B313D4466B91C846CCC18BA44B"
   },
   "source": [
    "We may test our implementation against Rémi Flamary's algorithms in [Python Optimal Transport](https://github.com/rflamary/POT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "0EDD7D978F64462880A2D1CFE6833739",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ot\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "F0772ABA3E824ADF8792A13CBAB82F1F",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test problem from Python Optimal Transport\n",
    "n=100\n",
    "a=ot.datasets.make_1D_gauss(n,m=20,s=10).astype(numpy.float32)\n",
    "b=ot.datasets.make_1D_gauss(n,m=60,s=30).astype(numpy.float32)\n",
    "c=ot.datasets.make_1D_gauss(n,m=40,s=20).astype(numpy.float32)\n",
    "a64=ot.datasets.make_1D_gauss(n,m=20,s=10).astype(numpy.float64)\n",
    "b64=ot.datasets.make_1D_gauss(n,m=60,s=30).astype(numpy.float64)\n",
    "c64=ot.datasets.make_1D_gauss(n,m=40,s=20).astype(numpy.float64)\n",
    "# distance function\n",
    "x=numpy.arange(n,dtype=numpy.float32)\n",
    "M=(x[:,numpy.newaxis]-x[numpy.newaxis,:])**2\n",
    "M/=M.max()\n",
    "x64=numpy.arange(n,dtype=numpy.float64)\n",
    "M64=(x64[:,numpy.newaxis]-x64[numpy.newaxis,:])**2\n",
    "M64/=M64.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "33772D05E8344EA590D3AA22F3B0709C",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transp = ot.bregman.sinkhorn(a,b,M,reg=1e-3)\n",
    "transp2 = ot.bregman.sinkhorn_stabilized(a,b,M,reg=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "D1952592EE634C83BE396862915C82F0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15025606400382638, 0.1502669613218494)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(transp*M).sum(), (transp2*M).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "C1078B34587145C981BD4818A57AE40D",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cabt = torch.from_numpy(numpy.stack((c,a,b),axis=0))\n",
    "abct = torch.from_numpy(numpy.stack((a,b,c),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "070CF22C32154E778C882A19143A22B3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1053]),\n",
       " (tensor([0.1053]), tensor([0.0758]), tensor([0.1773]), tensor([0.0628])))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossvanilla = WassersteinLossVanilla(torch.from_numpy(M), lam=0.1)\n",
    "loss = lossvanilla\n",
    "losses = loss(cabt,abct), loss(cabt[:1],abct[:1]), loss(cabt[1:2],abct[1:2]), loss(cabt[2:],abct[2:])\n",
    "sum(losses[1:])/3, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "5AF1937602A240BF95BCDFCC730175AA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1053]),\n",
       " (tensor([0.1053]), tensor([0.0758]), tensor([0.1773]), tensor([0.0628])))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = WassersteinLossStab(torch.from_numpy(M), lam=0.1)\n",
    "losses = loss(cabt,abct), loss(cabt[:1],abct[:1]), loss(cabt[1:2],abct[1:2]), loss(cabt[2:],abct[2:])\n",
    "sum(losses[1:])/3, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "701438D79C48420AB97E17AEF6C8B76E"
   },
   "source": [
    "The stabilized version can handle the extended range needed to get closer to the Python Optimal Transport loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "DC144D4A21074F0B9CF68E1006A8A781"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15445974773826976, 0.15445145964622498)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transp3 = ot.bregman.sinkhorn_stabilized(a,b,M,reg=1e-2)\n",
    "loss = WassersteinLossStab(torch.from_numpy(M), lam=0.01)\n",
    "(transp3*M).sum(), loss(cabt[1:2],abct[1:2]).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5A983A5E9B2C4A809C6AE0AE3B4704B7"
   },
   "source": [
    "By the linear expansion, we should have\n",
    "$$\n",
    "L(x2) \\approx L(x1)+\\nabla L(\\frac{x1+x2}{2})(x2-x1),\n",
    "$$\n",
    "so in particular we can see if for an example\n",
    "$L(x+\\epsilon \\nabla L)-L(x1) / \\epsilon \\|\\nabla L\\|^2 \\approx 1$.\n",
    "\n",
    "This seems to be the case ... sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "CDCD8C5F655248578FAE425282E913F5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9221])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theloss = WassersteinLossStab(torch.from_numpy(M), lam=0.01, sinkhorn_iter=50)\n",
    "cabt = torch.from_numpy(numpy.stack((c,a,b),axis=0))\n",
    "abct = torch.from_numpy(numpy.stack((a,b,c),axis=0)).requires_grad_()\n",
    "lossv1 = theloss(abct,cabt)\n",
    "lossv1.backward()\n",
    "grv = abct.grad\n",
    "epsilon = 1e-5\n",
    "abctv2 = (abct.data-epsilon*grv.data).requires_grad_()\n",
    "lossv2 = theloss(abctv2, cabt)\n",
    "lossv2.backward()\n",
    "grv2 = abctv2.grad\n",
    "(lossv1.data-lossv2.data)/(epsilon*((0.5*(grv.data+grv2.data))**2).sum()) # should be around 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "E1E7785FCF2E4754B2992D07ADBE636A"
   },
   "source": [
    "Naturally, one has to check whether the abctv2 is a valid probability distribution (i.e. all entries $>0$). It seems that the range of $\\lambda$ in which the gradient works well is somewhat limited. This may point to a bug in the implementation.\n",
    "\n",
    "Note also that feeding the same distribution in both arguments results in a NaN, when 0 is the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "734A542D44B14B86827DFAF41774BC2D"
   },
   "source": [
    "## Straightforward port of Python Optimal Transport's Sinkhorn routines\n",
    "\n",
    "These are more straightforward ports of Rémi Flamary's algorithms in [Python Optimal Transport](https://github.com/rflamary/POT) useful for investigating stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "E7C8C65A1F454D828D7CCEEE431EFFFC",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sinkhorn(a,b, M, reg, numItermax = 1000, stopThr=1e-9, verbose=False, log=False):\n",
    "    # seems to explode terribly fast with 32 bit floats...\n",
    "    if a is None:\n",
    "        a = M.new(M.size(0)).fill_(1/m.size(0))\n",
    "    if b is None:\n",
    "        b = M.new(M.size(0)).fill_(1/M.size(1))\n",
    "\n",
    "    # init data\n",
    "    Nini = a.size(0)\n",
    "    Nfin = b.size(0)\n",
    "\n",
    "    cpt = 0\n",
    "    if log:\n",
    "        log={'err':[]}\n",
    "\n",
    "    # we assume that no distances are null except those of the diagonal of distances\n",
    "    u = M.new(Nfin).fill_(1/Nfin)\n",
    "    v = M.new(Nfin).fill_(1/Nfin)\n",
    "    uprev=M.new(Nini).zero_()\n",
    "    vprev=M.new(Nini).zero_()\n",
    "\n",
    "    K = torch.exp(-M/reg)\n",
    "\n",
    "    Kp = K/(a[:,None].expand_as(K))\n",
    "    transp = K\n",
    "    cpt = 0\n",
    "    err=1\n",
    "    while (err>stopThr and cpt<numItermax):\n",
    "        Kt_dot_u = torch.mv(K.t(),u)\n",
    "        if (Kt_dot_u==0).sum()>0 or (u!=u).sum()>0 or (v!=v).sum()>0: # u!=u is a test for NaN...\n",
    "            # we have reached the machine precision\n",
    "            # come back to previous solution and quit loop\n",
    "            print('Warning: numerical errrors')\n",
    "            if cpt!=0:\n",
    "                u = uprev\n",
    "                v = vprev\n",
    "            break\n",
    "        uprev = u\n",
    "        vprev = v\n",
    "        v = b/Kt_dot_u\n",
    "        u = 1./torch.mv(Kp,v)\n",
    "        if cpt%10==0:\n",
    "            # we can speed up the process by checking for the error only all the 10th iterations\n",
    "            transp =   (u[:,None].expand_as(K))*K*(v[None,:].expand_as(K))\n",
    "            err = torch.dist(transp.sum(0),b)**2\n",
    "            if log:\n",
    "                log['err'].append(err)\n",
    "\n",
    "            if verbose:\n",
    "                if cpt%200 ==0:\n",
    "                    print('{:5s}|{:12s}'.format('It.','Err')+'\\n'+'-'*19)\n",
    "                print('{:5d}|{:8e}|'.format(cpt,err))\n",
    "        cpt = cpt +1\n",
    "    if log:\n",
    "        log['u']=u\n",
    "        log['v']=v\n",
    "    #print 'err=',err,' cpt=',cpt\n",
    "    if log:\n",
    "        return (u[:,None].expand_as(K))*K*(v[None,:].expand_as(K)),log\n",
    "    else:\n",
    "        return (u[:,None].expand_as(K))*K*(v[None,:].expand_as(K))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "6F7AB155A73144FE9F7C9274FAED8B99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: numerical errrors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.168404344971009e-18, 0.006920528598129749)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test 32 bit vs. 64 bit for unstabilized\n",
    "typ = numpy.float64\n",
    "dist_torch64 =  sinkhorn(torch.from_numpy(a.astype(typ)),torch.from_numpy(b.astype(typ)),\n",
    "                   torch.from_numpy(M.astype(typ)),reg=1e-3)\n",
    "typ = numpy.float32\n",
    "dist_torch32 =  sinkhorn(torch.from_numpy(a.astype(typ)),torch.from_numpy(b.astype(typ)),\n",
    "                   torch.from_numpy(M.astype(typ)),reg=1e-3)\n",
    "dist_pot     = ot.bregman.sinkhorn(a,b,M,reg=1e-3)\n",
    "numpy.abs(dist_torch64.numpy()-dist_pot).max(), numpy.abs(dist_torch32.numpy()-dist_pot).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_id": "D2D877D54B894CAA9E3F4EF4651C5409"
   },
   "outputs": [],
   "source": [
    "def sinkhorn_stabilized(a,b, M, reg, numItermax = 1000,tau=1e3, stopThr=1e-9,\n",
    "                        warmstart=None, verbose=False,print_period=20, log=False):\n",
    "    if a is None:\n",
    "        a = M.new(m.size(0)).fill_(1/m.size(0))\n",
    "    if b is None:\n",
    "        b = M.new(m.size(0)).fill_(1/m.size(1))\n",
    "\n",
    "    # init data\n",
    "    na = a.size(0)\n",
    "    nb = b.size(0)\n",
    "\n",
    "    cpt = 0\n",
    "    if log:\n",
    "        log={'err':[]}\n",
    "\n",
    "\n",
    "    # we assume that no distances are null except those of the diagonal of distances\n",
    "    if warmstart is None:\n",
    "        alpha,beta=M.new(na).zero_(),M.new(nb).zero_()\n",
    "    else:\n",
    "        alpha,beta=warmstart\n",
    "    u,v = M.new(na).fill_(1/na),M.new(nb).fill_(1/nb)\n",
    "    uprev,vprev=M.new(na).zero_(),M.new(nb).zero_()\n",
    "\n",
    "    def get_K(alpha,beta):\n",
    "        \"\"\"log space computation\"\"\"\n",
    "        return torch.exp(-(M-alpha[:,None].expand_as(M)-beta[None,:].expand_as(M))/reg)\n",
    "\n",
    "    def get_Gamma(alpha,beta,u,v):\n",
    "        \"\"\"log space gamma computation\"\"\"\n",
    "        return torch.exp(-(M-alpha[:,None].expand_as(M)-beta[None,:].expand_as(M))/reg+torch.log(u)[:,None].expand_as(M)+torch.log(v)[None,:].expand_as(M))\n",
    "\n",
    "    K=get_K(alpha,beta)\n",
    "    transp = K\n",
    "    loop=True\n",
    "    cpt = 0\n",
    "    err=1\n",
    "    while loop:\n",
    "\n",
    "        if  u.abs().max()>tau or  v.abs().max()>tau:\n",
    "            alpha, beta = alpha+reg*torch.log(u), beta+reg*torch.log(v)\n",
    "            u,v = M.new(na).fill_(1/na),M.new(nb).fill_(1/nb)\n",
    "            K=get_K(alpha,beta)\n",
    "\n",
    "        uprev = u\n",
    "        vprev = v\n",
    "        \n",
    "        Kt_dot_u = torch.mv(K.t(),u)\n",
    "        v = b/Kt_dot_u\n",
    "        u = a/torch.mv(K,v)\n",
    "\n",
    "        if cpt%print_period==0:\n",
    "            # we can speed up the process by checking for the error only all the 10th iterations\n",
    "            transp = get_Gamma(alpha,beta,u,v)\n",
    "            err = torch.dist(transp.sum(0),b)**2\n",
    "            if log:\n",
    "                log['err'].append(err)\n",
    "\n",
    "            if verbose:\n",
    "                if cpt%(print_period*20) ==0:\n",
    "                    print('{:5s}|{:12s}'.format('It.','Err')+'\\n'+'-'*19)\n",
    "                print('{:5d}|{:8e}|'.format(cpt,err))\n",
    "\n",
    "\n",
    "        if err<=stopThr:\n",
    "            loop=False\n",
    "\n",
    "        if cpt>=numItermax:\n",
    "            loop=False\n",
    "\n",
    "\n",
    "        if (Kt_dot_u==0).sum()>0 or (u!=u).sum()>0 or (v!=v).sum()>0: # u!=u is a test for NaN...\n",
    "            # we have reached the machine precision\n",
    "            # come back to previous solution and quit loop\n",
    "            print('Warning: numerical errrors')\n",
    "            if cpt!=0:\n",
    "                u = uprev\n",
    "                v = vprev\n",
    "            break\n",
    "\n",
    "        cpt = cpt +1\n",
    "    #print 'err=',err,' cpt=',cpt\n",
    "    if log:\n",
    "        log['logu']=alpha/reg+torch.log(u)\n",
    "        log['logv']=beta/reg+torch.log(v)\n",
    "        log['alpha']=alpha+reg*torch.log(u)\n",
    "        log['beta']=beta+reg*torch.log(v)\n",
    "        log['warmstart']=(log['alpha'],log['beta'])\n",
    "        return get_Gamma(alpha,beta,u,v),log\n",
    "    else:\n",
    "        return get_Gamma(alpha,beta,u,v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "9594702058E544338D7993C14EA649D6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3092044809370762e-13, 1.2340842108111982e-07)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test 32 bit vs. 64 bit for stabilized\n",
    "typ = numpy.float64\n",
    "dist_torch64 =  sinkhorn_stabilized(torch.from_numpy(a.astype(typ)),torch.from_numpy(b.astype(typ)),\n",
    "                   torch.from_numpy(M.astype(typ)),reg=1e-3)\n",
    "typ = numpy.float32\n",
    "dist_torch32 =  sinkhorn_stabilized(torch.from_numpy(a.astype(typ)),torch.from_numpy(b.astype(typ)),\n",
    "                   torch.from_numpy(M.astype(typ)),reg=1e-3)\n",
    "dist_pot     = ot.bregman.sinkhorn_stabilized(a,b,M,reg=1e-3)\n",
    "numpy.abs(dist_torch64.numpy()-dist_pot).max(), numpy.abs(dist_torch32.numpy()-dist_pot).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "0F60CCF5E5834867A94CE450B97F90FB",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
